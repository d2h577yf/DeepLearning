import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from typing import Tuple, List, Optional, Dict, Union, Callable
import time
import numpy as np


# ==================== å·¥å…·ç±» ====================

class Accumulator:
    """ç”¨äºç´¯ç§¯å¤šä¸ªå˜é‡çš„å’Œ"""

    def __init__(self, n: int):
        self.data = [0.0] * n

    def add(self, *args):
        """æ·»åŠ å€¼åˆ°ç´¯ç§¯å™¨"""
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        """é‡ç½®ç´¯ç§¯å™¨"""
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]


class ConsoleMonitor:
    """ç»ˆç«¯è®­ç»ƒç›‘æ§å™¨ï¼Œç”¨äºåœ¨ç»ˆç«¯æ˜¾ç¤ºè®­ç»ƒè¿›åº¦å’ŒæŒ‡æ ‡"""

    def __init__(self, num_epochs: int, show_progress_bar: bool = True):
        self.num_epochs = num_epochs
        self.show_progress_bar = show_progress_bar
        self.start_time = time.time()
        self.best_test_acc = 0.0
        self.best_epoch = 0

    def print_header(self):
        """æ‰“å°è¡¨å¤´"""
        print(f"{'Epoch':<10} {'Train Loss':<15} {'Train Acc':<15} {'Test Acc':<15} {'Time':<15}")
        print("-" * 70)

    def print_epoch(self, epoch: int, train_loss: float, train_acc: float, test_acc: float):
        """æ‰“å°æ¯ä¸ªepochçš„ç»“æœ"""
        epoch_time = time.time() - self.start_time

        # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
        if test_acc > self.best_test_acc:
            self.best_test_acc = test_acc
            self.best_epoch = epoch

        print(f"{epoch:<10} {train_loss:<15.4f} {train_acc:<15.4f} {test_acc:<15.4f} {epoch_time:<15.2f}s")

        # æ˜¾ç¤ºè¿›åº¦æ¡ï¼ˆå¯é€‰ï¼‰
        if self.show_progress_bar and epoch > 0:
            self._print_progress_bar(epoch)

    def _print_progress_bar(self, epoch: int):
        """æ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ¡"""
        progress = int((epoch / self.num_epochs) * 40)
        bar = "[" + "=" * progress + ">" + " " * (40 - progress) + "]"
        percent = (epoch / self.num_epochs) * 100
        print(f"è¿›åº¦: {bar} {percent:.1f}% ({epoch}/{self.num_epochs})")

    def print_summary(self, history: Dict):
        """æ‰“å°è®­ç»ƒæ€»ç»“"""
        print("\n" + "=" * 70)
        print("è®­ç»ƒå®Œæˆ!")
        print(f"æ€»è®­ç»ƒæ—¶é—´: {time.time() - self.start_time:.2f}ç§’")
        print(f"æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {self.best_test_acc:.4f} (ç¬¬{self.best_epoch}ä¸ªepoch)")

        # æ‰“å°æœ€ç»ˆç»“æœ
        print(f"\næœ€ç»ˆç»“æœ:")
        print(f"è®­ç»ƒæŸå¤±: {history['train_loss'][-1]:.4f}")
        print(f"è®­ç»ƒå‡†ç¡®ç‡: {history['train_acc'][-1]:.4f}")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {history['test_acc'][-1]:.4f}")

        # æ‰“å°æ”¹è¿›æƒ…å†µ
        if len(history['train_acc']) > 1:
            train_improvement = history['train_acc'][-1] - history['train_acc'][0]
            test_improvement = history['test_acc'][-1] - history['test_acc'][0]
            print(f"\næ”¹è¿›æƒ…å†µ:")
            print(f"è®­ç»ƒå‡†ç¡®ç‡æå‡: {train_improvement:+.4f}")
            print(f"æµ‹è¯•å‡†ç¡®ç‡æå‡: {test_improvement:+.4f}")

            # æ£€æŸ¥è¿‡æ‹Ÿåˆ
            train_test_gap = history['train_acc'][-1] - history['test_acc'][-1]
            if train_test_gap > 0.15:  # å¦‚æœè®­ç»ƒå‡†ç¡®ç‡æ¯”æµ‹è¯•å‡†ç¡®ç‡é«˜15%ä»¥ä¸Š
                print(f"âš ï¸  æ³¨æ„: å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ (è®­ç»ƒ-æµ‹è¯•å·®è·: {train_test_gap:.4f})")
            elif train_test_gap < -0.05:  # å¦‚æœæµ‹è¯•å‡†ç¡®ç‡æ¯”è®­ç»ƒå‡†ç¡®ç‡é«˜5%ä»¥ä¸Š
                print(f"âš ï¸  æ³¨æ„: å¯èƒ½å­˜åœ¨æ¬ æ‹Ÿåˆ (è®­ç»ƒ-æµ‹è¯•å·®è·: {train_test_gap:.4f})")

    def print_checkpoint_saved(self, save_path: str):
        """æ‰“å°æ¨¡å‹ä¿å­˜ä¿¡æ¯"""
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")


# ==================== æ¨¡å‹ç±»å‹æ£€æµ‹ ====================

def is_functional_model(model) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºå‡½æ•°å¼æ¨¡å‹ï¼ˆå‡½æ•° + å‚æ•°åˆ—è¡¨ï¼‰

    Args:
        model: æ¨¡å‹å¯¹è±¡

    Returns:
        bool: æ˜¯å¦ä¸ºå‡½æ•°å¼æ¨¡å‹
    """
    if isinstance(model, tuple) and len(model) == 2:
        # å¦‚æœæ˜¯(å‡½æ•°, å‚æ•°åˆ—è¡¨)çš„å…ƒç»„
        return callable(model[0]) and isinstance(model[1], list)
    elif callable(model):
        # å¦‚æœæ˜¯å‡½æ•°ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³çš„å…¨å±€å‚æ•°
        return True
    else:
        return False


def prepare_functional_model(model, device: torch.device):
    """
    å‡†å¤‡å‡½æ•°å¼æ¨¡å‹è¿›è¡Œè®­ç»ƒ

    Args:
        model: å‡½æ•°å¼æ¨¡å‹
        device: è®­ç»ƒè®¾å¤‡

    Returns:
        å‡†å¤‡å¥½è®­ç»ƒçš„å‡½æ•°å¼æ¨¡å‹
    """
    if isinstance(model, tuple):
        # æ¨¡å‹æ˜¯(å‡½æ•°, å‚æ•°åˆ—è¡¨)å½¢å¼
        forward_fn, params = model
        # å°†å‚æ•°ç§»åŠ¨åˆ°è®¾å¤‡
        params = [param.to(device) for param in params]
        return forward_fn, params
    else:
        # æ¨¡å‹æ˜¯å‡½æ•°ï¼Œéœ€è¦åœ¨å¤–éƒ¨å®šä¹‰å‚æ•°
        return model


# ==================== æ ¸å¿ƒå‡½æ•° ====================

def accuracy(y_hat: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
    """è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡"""
    if y_hat.dim() > 1 and y_hat.shape[1] > 1:
        # å¤šåˆ†ç±»ï¼šå–æœ€å¤§æ¦‚ç‡çš„ç±»åˆ«
        y_hat = y_hat.argmax(dim=1)
    cmp = y_hat.type(y.dtype) == y
    return cmp.type(y.dtype).sum()


def evaluate_accuracy(model, data_iter: DataLoader,
                      device: torch.device = None) -> float:
    """è¯„ä¼°æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡"""
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # åˆ¤æ–­æ¨¡å‹ç±»å‹
    is_functional = is_functional_model(model)

    metric = Accumulator(2)  # [æ­£ç¡®æ•°, æ€»æ•°]

    with torch.no_grad():
        for X, y in data_iter:
            X, y = X.to(device), y.to(device)

            # æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œå‰å‘ä¼ æ’­
            if is_functional:
                if isinstance(model, tuple):
                    forward_fn, params = model
                    y_hat = forward_fn(X, *params)
                else:
                    # å‡½æ•°å¼æ¨¡å‹ï¼Œç›´æ¥è°ƒç”¨
                    y_hat = model(X)
            elif isinstance(model, nn.Module):
                model.eval()
                y_hat = model(X)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {type(model)}")

            metric.add(accuracy(y_hat, y), y.numel())

    return metric[0] / metric[1]


def train_epoch(model, train_iter: DataLoader, loss_fn: nn.Module,
                optimizer: torch.optim.Optimizer, device: torch.device = None) -> Tuple[float, float]:
    """è®­ç»ƒä¸€ä¸ªepoch"""
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # åˆ¤æ–­æ¨¡å‹ç±»å‹
    is_functional = is_functional_model(model)

    if isinstance(model, nn.Module):
        model.train()

    metric = Accumulator(3)  # [æ€»æŸå¤±, æ­£ç¡®æ•°, æ ·æœ¬æ•°]

    for batch_idx, (X, y) in enumerate(train_iter):
        X, y = X.to(device), y.to(device)

        # å‰å‘ä¼ æ’­
        if is_functional:
            if isinstance(model, tuple):
                forward_fn, params = model
                y_hat = forward_fn(X, *params)
            else:
                y_hat = model(X)
        elif isinstance(model, nn.Module):
            y_hat = model(X)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {type(model)}")

        loss = loss_fn(y_hat, y)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # ç´¯ç§¯æŒ‡æ ‡
        metric.add(loss.item() * y.size(0), accuracy(y_hat, y), y.size(0))

        # æ¯10ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦
        if batch_idx % 10 == 0 and batch_idx > 0:
            avg_loss = metric[0] / metric[2]
            avg_acc = metric[1] / metric[2]
            print(f"  Batch {batch_idx}/{len(train_iter)} - Loss: {avg_loss:.4f}, Acc: {avg_acc:.4f}")

    # è¿”å›å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
    return metric[0] / metric[2], metric[1] / metric[2]


def train_model(model, train_iter: DataLoader, test_iter: DataLoader,
                loss_fn: nn.Module, optimizer: torch.optim.Optimizer,
                num_epochs: int = 10, device: torch.device = None,
                save_path: Optional[str] = None, show_progress_bar: bool = True,
                validate_every: int = 1) -> Dict:
    """
    è®­ç»ƒæ¨¡å‹ä¸»å‡½æ•°

    Args:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå¯ä»¥æ˜¯nn.Moduleã€å‡½æ•°æˆ–(å‡½æ•°, å‚æ•°)å…ƒç»„
        train_iter: è®­ç»ƒæ•°æ®è¿­ä»£å™¨
        test_iter: æµ‹è¯•æ•°æ®è¿­ä»£å™¨
        loss_fn: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        num_epochs: è®­ç»ƒè½®æ•°
        device: è®­ç»ƒè®¾å¤‡
        save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        show_progress_bar: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        validate_every: æ¯éš”å¤šå°‘ä¸ªepochéªŒè¯ä¸€æ¬¡

    Returns:
        dict: åŒ…å«è®­ç»ƒå†å²çš„å­—å…¸
    """
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # åˆ¤æ–­æ¨¡å‹ç±»å‹
    is_functional = is_functional_model(model)
    is_module = isinstance(model, nn.Module)

    if is_functional:
        print("ğŸ¯ æ£€æµ‹åˆ°å‡½æ•°å¼æ¨¡å‹")
        if isinstance(model, tuple):
            forward_fn, params = model
            params = [param.to(device) for param in params]
            model = (forward_fn, params)
    elif is_module:
        print("ğŸ§  æ£€æµ‹åˆ°æ ‡å‡†nn.Moduleæ¨¡å‹")
        model = model.to(device)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {type(model)}")

    history = {'train_loss': [], 'train_acc': [], 'test_acc': [], 'epoch_times': []}

    # åˆå§‹åŒ–æ§åˆ¶å°ç›‘æ§å™¨
    monitor = ConsoleMonitor(num_epochs, show_progress_bar)

    print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œä½¿ç”¨è®¾å¤‡: {device}")
    print(f"ğŸ“Š æ€»epochæ•°: {num_epochs}, æ‰¹é‡å¤§å°: {train_iter.batch_size}")
    monitor.print_header()

    for epoch in range(1, num_epochs + 1):
        epoch_start_time = time.time()

        # è®­ç»ƒä¸€ä¸ªepoch
        train_loss, train_acc = train_epoch(model, train_iter, loss_fn, optimizer, device)

        # è®°å½•epochæ—¶é—´
        epoch_time = time.time() - epoch_start_time
        history['epoch_times'].append(epoch_time)

        # æ¯éš”validate_everyä¸ªepochè¯„ä¼°ä¸€æ¬¡æµ‹è¯•é›†
        if epoch % validate_every == 0 or epoch == num_epochs:
            test_acc = evaluate_accuracy(model, test_iter, device)
        else:
            test_acc = history['test_acc'][-1] if history['test_acc'] else 0.0

        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['test_acc'].append(test_acc)

        # æ‰“å°epochç»“æœ
        monitor.print_epoch(epoch, train_loss, train_acc, test_acc)

        # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
        if save_path and epoch % 5 == 0:
            checkpoint_path = f"{save_path}_epoch_{epoch}.pth"
            # æ ¹æ®æ¨¡å‹ç±»å‹ä¿å­˜
            if is_module:
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'train_loss': train_loss,
                    'train_acc': train_acc,
                    'test_acc': test_acc
                }, checkpoint_path)
            else:
                print(f"âš ï¸  å‡½æ•°å¼æ¨¡å‹æ— æ³•ä¿å­˜ä¸ºæ ‡å‡†çš„PyTorchæ¨¡å‹æ ¼å¼ï¼Œè·³è¿‡ä¿å­˜")

            monitor.print_checkpoint_saved(checkpoint_path)

    # è®­ç»ƒå®Œæˆï¼Œæ‰“å°æ€»ç»“
    monitor.print_summary(history)

    # æœ€ç»ˆä¿å­˜æ¨¡å‹
    if save_path:
        if is_module:
            final_save_path = f"{save_path}_final.pth" if not save_path.endswith('.pth') else save_path
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'history': history,
                'num_epochs': num_epochs,
                'final_train_acc': history['train_acc'][-1],
                'final_test_acc': history['test_acc'][-1]
            }, final_save_path)
            print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_save_path}")
        else:
            print("âš ï¸  å‡½æ•°å¼æ¨¡å‹æ— æ³•ä¿å­˜ä¸ºæ ‡å‡†çš„PyTorchæ¨¡å‹æ ¼å¼")
            print("   è¯·æ‰‹åŠ¨ä¿å­˜æ¨¡å‹å‚æ•°")

    print("\nâœ… è®­ç»ƒå®Œæˆ!")
    return history


def predict(model, data_iter: DataLoader,
            num_samples: int = 10, class_names: List[str] = None,
            device: torch.device = None) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    å¯¹æ•°æ®é›†è¿›è¡Œé¢„æµ‹

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        data_iter: æ•°æ®è¿­ä»£å™¨
        num_samples: æ˜¾ç¤ºçš„æ ·æœ¬æ•°é‡
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
        device: æ¨ç†è®¾å¤‡

    Returns:
        Tuple: (é¢„æµ‹ç»“æœ, çœŸå®æ ‡ç­¾)
    """
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # åˆ¤æ–­æ¨¡å‹ç±»å‹
    is_functional = is_functional_model(model)

    if isinstance(model, nn.Module):
        model.eval()

    # è·å–ä¸€ä¸ªbatchçš„æ•°æ®
    for X, y in data_iter:
        break

    X, y = X.to(device), y.to(device)

    # é¢„æµ‹
    with torch.no_grad():
        if is_functional:
            if isinstance(model, tuple):
                forward_fn, params = model
                y_hat = forward_fn(X, *params)
            else:
                y_hat = model(X)
        elif isinstance(model, nn.Module):
            y_hat = model(X)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {type(model)}")

        preds = y_hat.argmax(dim=1) if y_hat.dim() > 1 else y_hat

    # é™åˆ¶æ˜¾ç¤ºæ•°é‡
    num_samples = min(num_samples, X.size(0))

    # å¦‚æœæ²¡æœ‰æä¾›ç±»åˆ«åç§°ï¼Œä½¿ç”¨æ•°å­—æ ‡ç­¾
    if class_names is None:
        class_names = [str(i) for i in range(10)]

    # æ‰“å°é¢„æµ‹ç»“æœ
    print(f"\nğŸ” é¢„æµ‹ç»“æœ (æ˜¾ç¤ºå‰{num_samples}ä¸ªæ ·æœ¬):")
    print(f"{'æ ·æœ¬':<10} {'é¢„æµ‹':<15} {'çœŸå®':<15} {'çŠ¶æ€':<15}")
    print("-" * 55)

    correct_count = 0
    for i in range(num_samples):
        pred_label = preds[i].item()
        true_label = y[i].item()

        # è·å–ç±»åˆ«åç§°
        pred_name = class_names[pred_label] if pred_label < len(class_names) else str(pred_label)
        true_name = class_names[true_label] if true_label < len(class_names) else str(true_label)

        is_correct = pred_label == true_label
        status = "âœ… æ­£ç¡®" if is_correct else "âŒ é”™è¯¯"

        if is_correct:
            correct_count += 1

        print(f"{i+1:<10} {pred_name:<15} {true_name:<15} {status:<15}")

    # è®¡ç®—å¹¶æ˜¾ç¤ºbatchå‡†ç¡®ç‡
    total_correct = (preds == y).sum().item()
    total = y.size(0)
    accuracy = total_correct / total

    print(f"\nğŸ“Š å½“å‰batchç»Ÿè®¡:")
    print(f"  æ ·æœ¬æ€»æ•°: {total}")
    print(f"  æ­£ç¡®é¢„æµ‹: {total_correct}")
    print(f"  å‡†ç¡®ç‡: {accuracy:.2%}")
    print(f"  æ˜¾ç¤ºæ ·æœ¬æ­£ç¡®ç‡: {correct_count}/{num_samples} ({correct_count/num_samples:.2%})")

    return preds, y


def print_model_info(model):
    """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
    print("\nğŸ“‹ æ¨¡å‹ä¿¡æ¯:")

    is_functional = is_functional_model(model)

    if is_functional:
        print("  ç±»å‹: å‡½æ•°å¼æ¨¡å‹")
        if isinstance(model, tuple):
            forward_fn, params = model
            print(f"  å‚æ•°æ•°é‡: {len(params)}")
            for i, param in enumerate(params):
                print(f"    å‚æ•°{i}: shape={param.shape}, dtype={param.dtype}, requires_grad={param.requires_grad}")
    elif isinstance(model, nn.Module):
        print("  ç±»å‹: æ ‡å‡†nn.Module")
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"  æ€»å‚æ•°æ•°é‡: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"  ä¸å¯è®­ç»ƒå‚æ•°: {total_params - trainable_params:,}")

        # æ‰“å°å±‚ä¿¡æ¯
        print("\n  å±‚ä¿¡æ¯:")
        for name, module in model.named_children():
            num_params = sum(p.numel() for p in module.parameters())
            print(f"    {name}: {module.__class__.__name__}, å‚æ•°: {num_params:,}")
    else:
        print(f"  ç±»å‹: {type(model)}")
